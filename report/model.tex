\part{Construction du modèle}

    \chapter{Prototypage}
    \label{prototypes}

    \section{Premier prototype : modèle simple \og ouvert \fg}
        
        \subsection{Principes généraux}

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.9\linewidth]{img/open_model.png}
                \end{center}
                \caption{Schéma de principe du \og modèle ouvert \fg}
                \label{fig:open_model}
            \end{figure}     

            Le fonctionnement global de ce premier modèle (présenté à la \reffig{fig:open_model}) ne respecte pas les principes du machine learning.
            Il permet juste d'éprouver la méthode pressentie, ainsi que de se faire une idée de l'efficacité d'un modèle de ce type.
            En effet, même si on utilise des fonctionnalités d'extraction de features depuis des textes classiques dans des modèles de machine learning, il manque une partie de mesure de la performance, indispensable pour pouvoir évaluer et améliorer la pertinence du modèle.
            Les illustrations de ce chapitre sont issues du notebook présenté en annexe \mref{code:open_model}, et le code des classes utilisées (IngredientExtractor et PIMIngredientExtractor) est inclus dans le module pimest, en annexe \mref{code:pimest}.

            La classe PIMIngredientExtractor est juste un habillage du CountVectorizer de la bibliothèque scikit-learn et du Requester construit dans le module pimapi, permettant d'aller récupérer les données du PIM.
            Ce modèle met en application les principes du \og Bag Of Words \fg \cite{bag_of_words_wiki}, à savoir que :
            \begin{itemize}
                \item un vocabulaire est établi à partir de l'ensemble des différents mots du corpus de listes d'ingrédients
                \item chaque liste d'ingrédients est transformée en un vecteur de nombres entiers qui a la même longueur que le vocabulaire, ou chaque entier est le compte du nombre d'occurence du mot dans le vocabulaire
            \end{itemize}

            Ce modèle n'utilise pas les données étiquetées manuellement (présentées à la section \mref{manually_labelled_data}), mais se base simplement sur les listes d'ingrédients du PIM.
            L'hypothèse qui est faite avec ce modèle est la suivante : bien que non parfaitement en qualité (i.e. exactement identiques au contenu des pièces jointes, cf. la comparaison entre la ground truth et le contenu du PIM en section \mref{ingredient_comparison}), les listes d'ingrédients du PIM sont des textes dont le contenu est très similaire à une liste d'ingrédients.
            Un survol rapide de quelques exemples montre que cette hypothèse semble vérifiée (cf. \reftable{tbl:exemple_ingred}).

        \subsection{Entraînement}

            \subsubsection{Périmètre du set d'entraînement}
            
            Pour l'entraînement de ce modèle, on va uniquement se limiter aux produits d'épicerie ou de boissons non-alcoolisées.
            En effet, ce sont pour ces produits que la réglementation impose d'afficher en clair la composition aux consommateurs.
            On se limitera aussi aux produits qui portent une liste d'ingrédients, et qui sont \og En qualité \fg (cf. les définitions données à la section \mref{statuts} sur les statuts des produits).
            Sur les 13 000+ produits présents dans le PIM, environ 3 400 font partie du périmère et 9 800 en sont exclus.

            \subsubsection{Constitution du vocabulaire}

            La constitution du vocabulaire se fait de manière très basique : 
            \begin{itemize}
                \item les listes d'ingrédients sont mises en minuscules
                \item les mots sont ensuite séparés aux whitespaces (espaces, retours à la ligne, tabulations, \dots) et marques de ponctuation
            \end{itemize}
            Pour parvenir à ce résultat, on applique simplement la méthode fit de la classe CountVectorizer de la bibliothèque scikit-learn.
            Aucunes des autres fonctionnalités standard (retrait des accents, gestion des stopwords, prise en compte des ngrams) n'a été utilisée.
            Lors de cet apprentissage, le vocabulaire obtenu a une longueur de 2 500 mots environ, et les mots les plus fréquents sont (ces valeurs sont susceptibles de changer à la marge par rapport au notebook en annexe, avec les changements potentiels sur les données au sein du PIM):
            \begin{itemize}
                \item de     : 11419 occurences
                \item sucre  :  2057 occurences 
                \item sel    :  1669 occurences
                \item eau    :  1288 occurences
                \item acide  :  1241 occurences
                \item lait   :  1215 occurences
                \item huile  :  1214 occurences
                \item poudre :  1100 occurences
                \item en     :   962 occurences
                \item arôme  :   938 occurences
            \end{itemize}
            Les stopwords 'de' et 'en' apparaissent dans les mots les plus fréquents.
        
        \subsection{Prédiction}
            
            \subsubsection{Parsing des fiches techniques}

                Les fiches techniques sont extraites sous formes de binaires depuis de le PIM via des appels API.
                On utilise ensuite la bibliothèque PDFMiner.six pour récupérer le texte de la fiche technique sous forme d'un long string.
                Cette étape fait appel à la méthode statique PDFDecoder.content du module pimpdf, présenté en annexe \mref{code:pimpdf}.
                On découpe ensuite ce long string en une liste de strings plus court, en considérant comme séparateur la présence de deux retours à la ligne consécutifs.
                Ce choix de séparateur a été fait car les textes produits par PDFMiner.six portent des retours à la ligne à la fin de chaque ligne typographiée, même si la phrase continue sur la ligne suivante.
                Cette étape fournit pour chaque fiche technique le texte contenu sous forme d'une liste de strings (les blocs de texte de la \reffig{fig:open_model}).

            \subsubsection{Sélection du meilleur candidat}
            \label{open_model_similarity}

                \paragraph{Principe}
                L'identification du meilleur candidat parmi les blocs de texte se fait de la manière suivante :
                \begin{itemize}
                    \item on calcule une similarité avec le vocabulaire des ingrédients pour chacun des blocs de texte
                    \item on retourne le bloc de texte avec la similarité la plus élevée
                \end{itemize}
                La formule de calcul de la similarité utilisée dans ce modèle a été trouvée par chance, mais elle donne des résultats plutôt encourageants.
                D'autres modes de calcul de la similarité seront présentés dans la suite de ce rapport.

                La formule de calcul de la similarité est :
                \[\frac{\text{Nombre de mots du candidat qui sont des ingrédients}}{\text{Norme euclidienne du vecteur du candidat}}\]
                
                Attention, la norme du vecteur du candidat s'entend \emph{indépendamment du vocabulaire des ingrédients}, c'est à dire que tous les mots comptent, y compris ceux n'appartenant pas au vocabulaire.

                \paragraph{Exemple}
                Si on illustre par un exemple fictif : imaginons que le vocabulaire des ingrédients soit composé des mots \emph{\og eau \fg, \og sucre \fg et \og farine \fg}.
                Le bloc de texte candidat est \emph{\og Sucre et farine sont biologiques. La farine est équitable.\fg}
                Si on vectorise ce bloc de texte \emph{sur son propre vocabulaire}, on obtient le vecteur suivant : 

                \bigskip
                \begin{minipage}{\textwidth}
                \captionsetup{type=table}
                \centering
                \begin{tabular}{cccccccc}
                    \toprule
                    sucre & et & farine & sont & biologiques & la & est & équitable \\
                    \midrule
                    1 & 1 & 2 & 1 & 1 & 1 & 1 & 1 \\
                    \bottomrule
                \end{tabular}
                \caption{Exemple de vectorisation d'un texte}
                \bigskip
                \end{minipage}                

                Sa norme euclidienne~\cite{norm_wiki} se calcule de la manière suivante (en notant $x_{i}$ le compte des mots) : 
                \[\sqrt{\sum_{i}^{} x_{i}^{2}} = \sqrt{1 + 1 + 4 + 1 + 1 + 1 + 1 + 1} \approx 3.317\]

                Le bloc contient 9 mots, dont 3 sont des ingrédients (\og farine \fg est mentionné deux fois). La similarité pour cet exemple vaut donc :
                \[\frac{3}{3.317} \approx 0.905\]
                \'{E}tant donné son mode de calcul, cette similarité peut tout à fait être supérieure à 1.

        \subsection{Illustration des résultats obtenus}
    
            \subsubsection{Périmètre du test}
            
            Pour éviter de surestimer la performance du modèle, on le fait tourner sur des produits n'ont pas fait partie du set d'entraînement.            
            Comme ce modèle ne permet de toute façon pas de fournir de résultats qui permettent de mesurer la performance, on ne le fera tourner que sur un échantillon réduit de produits, soit 5 d'entre eux.

            \subsubsection{Résultats}

            Les résultats obtenus sont les suivants :

                \begin{spacing}{1.0}
                \label{blocks_examples}
                {\scriptsize
                {\ttfamily 
                \begin{spverbatim}
Fetching data from PIM for uid d9b233a6-b455-4af6-afb4-623f1f7f62a6...
Done
----------------------------------------------------------
Ingredient list from PIM is :

Ingrédients: Huile de tournesol, oignon, curry (11,2%) (ail, coriandre, curcuma, gingembre, paprika, poivre, cumin, poivre de Cayenne, fenouil, cardamome, noix de muscade, canelle, clous de girofle, safran), pomme, sel, exhausteur de goût (glutamate de sodium), sucre, huile de colza totalement hydrogénée, extrait de levure, ail.
----------------------------------------------------------
Downloading content of technical datasheet file...
Done!
----------------------------------------------------------
Parsing content of technical datasheet file...
Done!
----------------------------------------------------------
Ingredient list extracted from technical datasheet:

Liste d’ingrédients : Huile de tournesol, oignon, curry (11,2%) (ail, coriandre, curcuma, gingembre, paprika, poivre, cumin,
poivre de Cayenne, fenouil, cardamome, noix de muscade, canelle, clous de girofle, safran), pomme, sel, exhausteur de goût
(glutamate de sodium), sucre, huile de colza totalement hydrogénée, extrait de levure, ail
----------------------------------------------------------

=======================================================================
=======================================================================
Fetching data from PIM for uid 5666235b-9e78-44f2-8e0e-1de53f88fe04...
Done
----------------------------------------------------------
Ingredient list from PIM is :

Ingrédients : Sucre, Gomme base, Sirop de glucose, Arômes, Humectant (E422), Antioxydant (E321), Colorant (E141).
----------------------------------------------------------
Downloading content of technical datasheet file...
Done!
----------------------------------------------------------
Parsing content of technical datasheet file...
Done!
----------------------------------------------------------
Ingredient list extracted from technical datasheet:

INGRÉDIENTS : Sucre, Gomme base, Sirop de glucose, Arômes, Humectant (E422), 
Antioxydant (E321), Colorant (E141).
----------------------------------------------------------

=======================================================================
=======================================================================

Fetching data from PIM for uid 6e976147-adeb-4d2d-925a-cb7c58c111a2...
Done
----------------------------------------------------------
Ingredient list from PIM is :

Maltodextrine, amidon de maïs, sel, farine de BLE, colorant : caramel ordinaire ; arômes (BLE,CELERI), huile de palme, épaississant : gomme guar ; oignon, fécule de pomme de terre, extrait de levure, jus de cuisson de viande de boeuf (0,9%), acidifiant : acide citrique ; extrait de vin blanc, extraits d'ail, de thym et de poivre. Peut contenir : LAIT, OEUF.
----------------------------------------------------------
Downloading content of technical datasheet file...
Done!
----------------------------------------------------------
Parsing content of technical datasheet file...
Done!
----------------------------------------------------------
Ingredient list extracted from technical datasheet:

Maltodextrine, amidon de maïs, sel, farine de blé, colorant : caramel ordinaire ; arômes (blé,céleri), huile de palme, épaississant : gomme guar ; oignon, fécule 
de pomme de terre, extrait de levure, jus de cuisson de viande de bœuf (0,9%), acidifiant : acide citrique ; extrait de vin blanc, extraits d'ail, de thym et de poivre. 
Peut contenir : lait, œuf.

----------------------------------------------------------

=======================================================================
=======================================================================

Fetching data from PIM for uid db449562-d16d-4f72-b7a5-c0d487bc8206...
Done
----------------------------------------------------------
Ingredient list from PIM is :

Huile d'ARACHIDE

----------------------------------------------------------
Downloading content of technical datasheet file...
Done!
----------------------------------------------------------
Parsing content of technical datasheet file...
Done!
----------------------------------------------------------
Ingredient list extracted from technical datasheet:

*Selon le règlement UE n°1259-2011 / In accordance with regulation UE n°1259-2011  
    
    
CARACTERISTIQUES MICROBIOLOGIQUES 
    
L’huile étant un milieu anhydre, tout développement bactérien est impossible (cf. ouvrage de 
référence  dans  ce  domaine  "La  qualité  microbiologique  des  aliments"  CNERMA-CNRS 
coordonné par Jean-louis Jouve). 
    
ORIGINES/ ORIGIN 
    
- Amérique du Sud majoritairement 
- Afrique de l’Ouest 
    
AUTRES INFORMATIONS 
    

----------------------------------------------------------

=======================================================================
=======================================================================

Fetching data from PIM for uid f2af54a2-6820-4f1b-99e7-d6e64642bdf3...
Done
----------------------------------------------------------
Ingredient list from PIM is :

None

----------------------------------------------------------
Downloading content of technical datasheet file...
Done!
----------------------------------------------------------
Parsing content of technical datasheet file...
Done!
----------------------------------------------------------
Ingredient list extracted from technical datasheet:

mini 16,56 ( - 10 % )

----------------------------------------------------------

=======================================================================
=======================================================================  
                \end{spverbatim}
                }
                }
                \end{spacing}

            \subsubsection{Pistes d'améliorations identifiées}

            En plus de la mesure de la performance, qui est indispensable avant de pouvoir procéder à des ajustements, les pistes identifiées à sont les suivantes :
            \begin{itemize}
                \item Faire un découpage plus élaboré du texte des pièces jointes en blocs, potentiellement avec des expressions régulières
                \item Essayer d'autres manières de vectoriser les textes (utiliser un comptage de mots binaire : présence/absence, calculer l'inverse document frequency, \dots)
                \item Essayer une autre manière de calculer la similarité
                \item Utiliser des fonctions plus élaborées de text preprocessing (retrait des stopwords, \dots)
            \end{itemize}

    \section{Second prototype : industrialisation}

            % REPRENDRE LA RELECTURE ICI

    L'objectif de ce second prototype est de permettre est d'industrialiser un peu le code afin : 
    \begin{itemize}
        \item d'être en mesure inspecter les résultats obtenus et pouvoir donner des explications sur la réponse donnée par le modèle
        \item de pouvoir utiliser des fonctionnalités de mesure de la performance du modèle
        \item d'être en mesure de faire varier les paramètres du modèle en vue de l'optimiser
    \end{itemize}
    Comme présenté à la section \mref{ingredient_comparison} relative à la comparaison entre les données du PIM et celles récupérées lors de l'étiquetage, il y a un grand nombre d'écarts.
    Or, si on entraîne le modèle et qu'on mesure sa performance sur des données de mauvaise qualité, on aura de mauvais résultats.
    Ce second modèle se basera sur les données manuellement étiquetées, afin d'avoir une base de comparaison dans la mesure de la performance.
    Le fonctionnement de ce modèle est présentés à la \reffig{fig:ground_truth_model}.
    La méthodologie utilisée à cette partie est présentée dans le notebook \og Modèle basé sur les données manuellement étiquetées \fg en annexe \mref{code:gt_based_model}.
    Les différents transformateurs et estimateurs spécifiques sont définis dans le module pimest, inclu en annexe \mref{code:pimest}.
    
    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=0.9\linewidth]{img/ground_truth_model.png}
        \end{center}
        \caption{Schéma de principe du modèle basé sur les données étiquetées}
        \label{fig:ground_truth_model}
    \end{figure}     

        \subsection{Chargement des données manuellement étiquetées}

        La toute première étape est la constution d'un dataframe contenant : 
        \begin{itemize}
            \item les uid pour indexer les produits
            \item les listes d'ingrédients manuellement étiquetées depuis les fiches techniques
            \item le contenu de chacune des fiches techniques au format texte
        \end{itemize}
        On commence par charger les données du fichier csv contenant les uid et les listes d'ingrédients.
        Ensuite, un pipeline scikit learn d'acquisition des données est lancé.
        Il s'agit de 4 transformateurs en série, qui effectuent les travaux suivants :
        \begin{itemize}
            \item construction du chemin pointant vers les fiches techniques (sur la base des uid)
            \item construction d'une feature contenant les données des fichiers, en binaire
            \item construction du texte complet de la fiche technique (en se basant sur la library pdfminer.six)
        \end{itemize}
        Le résultat du lancement de ce pipeline est présenté à la \reftable{tbl:mod_GT_fulltexts}.

        {\renewcommand{\arraystretch}{1.5}%
        \begin{table}[htbp]
            \begin{center}
            {\scriptsize
            \input{tbls/processed_FT.tex}
            }
            \caption{Exemples du contenu de fiches techniques au format texte (tronqués)}
            \label{tbl:mod_GT_fulltexts}
            \end{center}
        \end{table}
        }

        \subsection{Découpage des textes en blocs}

        Le second travail est le découpage des textes en blocs.
        Cette étape étant peu coûteuse en temps de calcul, elle est réalisée sur l'ensemble du dataset avant le split entre set d'entrainement et set de test.
        Dans un premier temps, on va simplement effectuer ce découpage en coupant le texte lorsque deux retours à la ligne successifs sont détectés, comme cela avait été fait pour le premier prototype.
        Il est maintenant possible d'inspecter les résultats de cette étape, un exemple de découpage est présenté ci-dessous.

        \begin{multicols}{3}
        \begin{spacing}{1.0}
        \label{blocks_examples}
        {\tiny
        \input{tbls/block_example.tex}
        }
        \end{spacing}
        \end{multicols}

        On constate que le découpage n'est pas idéal, cf. la fiche technique de ce produit, présentée en annexe \mref{ex:FT_meltrappeur}.
        Les séparations des cellules des tableaux de cette fiche ne sont pas prises en compte, et on a des blocs trop étendus.

        \subsection{Entrainement et prédiction}

        Dans la mesure où l'on possède assez peu de données, on va conserver un échantillon assez important dans le jeu d'entraînement : 400 produits (soit 80\% des données disponibles).
        Comme cela a été présenté ci-dessus, l'entraînement ne consiste qu'en la détermination du vocabulaire des ingrédients à partir des textes manuellement étiquetés.        
        On fait tourner de la même manière que sur le modèle dit \og ouvert \fg, à savoir qu'on ne préprocesse que peu les données avant d'appliquer le CountVectorizer.
        Sur le set d'entraînement retenu, on a un vocabulaire d'ingrédients représentant 1204 mots.

        La prédiction du meilleur candidat se fait de la même manière que dans le cadre du prototype précédent (cf. section \mref{open_model_similarity}): on compare la norme du vecteur du bloc avec le nombre de mots appartenant au vocabulaire des ingrédients.
        La représentation des textes sous forme de vecteur se fait simplement via les comptes de chacun des mots du texte.

        \subsection{Illustration des prédictions obtenues}
        \label{prediction_gt_illustration}
        Un échantillon des prédictions obtenues est présenté dans la \reftable{tbl:GT_prediction_sample}.
        Pour éviter d'avoir des listes d'ingrédients prédites prenant trop de place dans cette table, celles dont la longueur dépasse 500 caractères ont été filtrées avant génération de cet échantillon.
        Les résultats présentés à cette table sont donc vraisemblablement biaisés, dans la mesure où les très longues listes prédites doivent avoir plus de chance d'être erronées.
        Les grandes tendances qui se dégagent à l'analyse de cette liste sont les suivantes : 
        \begin{itemize}
            \item globalement, les résultats sont bons. On retrouve régulièrement des morceaux de texte qui sont similaires à la liste cible
            \item une erreur qui revient régulièrement est le fait que le découpage en blocs est parfois imparfait, on sélectionne \og trop large \fg
            \item à l'inverse, le modèle n'a pas retiré des listes d'ingrédients prédites des mentions qui ont été rétirées lors de l'étiquetage manuel (cf. les règles d'annotation présentées en annexe \mref{annotation_rules}) : les préfixes de type \og Liste d'ingrédients : \fg, les allégations telles que \og Teneur totale en sucres : 60g pour 100g \fg \dots
            \item le modèle semble plus perfomant lorsque la liste d'ingrédients réelle est longue. On le vérifiera dans le chapitre relatif à la mesure de la performance du modèle
        \end{itemize}
        Un mot sur les cas où la liste d'ingrédients cible ou prédite sont vides :
        \begin{itemize}
            \item Les listes d'ingrédients cible sont vides lorsque la pièce jointe ne mentionnait pas de liste d'ingrédients. Cela peut arriver, et les produits concernés n'ont pas été sortis de l'échantillon. Il est important de pouvoir aussi mesurer les faux positifs, qui sont nombreux avec cette techniques de choix systématique du meilleur candidat
            \item Les liste d'ingrédient prédites sont vides lorsque l'outil de parsing des pdf (pdfminer.six) n'a extrait aucun texte. C'est le cas quand la pièce jointe était un document imprimé qui a été scanné. Le texte n'est présent que sous forme d'image (cf. la fiche technique du sel en annexe \mref{ex:FT_sel})
        \end{itemize}

        {\renewcommand{\arraystretch}{1.5}%
        \begin{spacing}{1.0}
        \begin{center}
            {\scriptsize
            \input{tbls/GT_prediction_sample.tex}
            }
        \end{center}
        \end{spacing}
        }

    \chapter{Mesure de la performance}
    \label{performance}
    
    Comme vu aux chapitre précédents, il est indispensable de mesurer la performance de nos modèles.
    On le fera sur la base du modèle précédent, se basant sur les données manuellement étiquetées.
    Le principe est présenté à la \reffig{fig:measured_model}.

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=0.9\linewidth]{img/measured_model.png}
        \end{center}
        \caption{Illustration de la méthodologie de mesure de la performance}
        \label{fig:measured_model}
    \end{figure}     

        \section{Accuracy}
        
        La métrique qui tombe le plus sous le sens est l'accuracy (on utilisera le terme anglais pour éviter les confusions avec la notion de \og precision \fg telle qu'elle est utilisée par exemple dans le f1-score).
        On mesure simplement la proportion de prédictions qui sont égales à la ground truth.
        La méthodologie utilisée est détaillée dans le notebook \og Mesure de la performance \fg présenté en annexe \mref{code:performance_measurement}.

            \subsection{Approche naïve}

                \subsubsection{Description de cette approche}

                L'approche \og naïve \fg consiste simplement à mesurer la proportion de textes prédits strictement égaux à la ground truth.
                Or, comme cela a été vu précédemment : 
                \begin{itemize}
                    \item les règles d'étiquetage manuel, détaillées à l'annexe \mref{annotation_rules}, montrent que des transformations sont parfois appliquées au contenu des listes d'ingrédients des pièces jointes avant d'établir la ground truth
                    \item les textes à comparer sont longs (jusqu'à quelques centaines de caractères), cf. \reftable{tbl:GT_prediction_sample}
                    \item la mise en forme, en particulier les retours à la ligne, ne sont pas positionnés aux mêmes endroits. Dans le parsing des documents pdf, lorsque le texte revient à la ligne après avoir atteint le bord de la page, l'outil positionne un retour à la ligne. Ce comportement n'a pas été reproduit dans l'établissement de la ground truth
                    \item le découpage en blocs de texte, de manière simple, produit des textes qui ne sont pas toujours le reflet du contenu spatialisé de ce document (cf. l'exemple donné à la section \mref{blocks_examples}, et la fiche technique associée en annexe \mref{ex:FT_meltrappeur})
                \end{itemize}
                On s'attend donc à avoir une \og accuracy naïve \fg faible.

                \subsubsection{Les résultats obtenus}

                Comme présenté dans notebook \og Analyse de la performance \fg (cf. annexe \mref{code:performance_measurement}), les résultats obtenus sont conformes à l'attendu.
                L'accuracy est très faible : 1\% (1 échantillon sur 100 produits) lorsqu'on la mesure sur l'échantillon de test après entraînement sur l'échantillon d'entraînement.
                Le seul produit pour lequel la liste d'ingrédients a été correctement identifiée porte les ingrédients suivants.
                \begin{quotation}
                    Sirop de glucose, sucre, eau, stabilisants (E440i, E440ii, E415), acidifiants (E330, E450i), conversateur (E202).
                \end{quotation}
                Si on effectue une cross-validation sur l'ensemble des données étiquetées, en appliquant un découpage en 10 folds, on obtient une accuracy moyenne de $1.80\% \pm 1.89\%$.

                \subsubsection{Nécessité d'améliorer cette métrique}
                On a vu que l'accuracy calculée de manière naïve porte un jugement sévère sur la performance du modèle.
                Par exemple, à la troisième ligne de la \reftable{tbl:GT_prediction_sample}, on voit bien que la liste d'ingrédients prédite est identique à la ground truth, si ce n'est qu les retours à la ligne ne sont pas positionnés exactement au même endroit.
                Comme on souhaite pouvoir ajuster le modèle, il est nécessaire d'avoir une métrique de mesure de la performance qui soit plus précise.

            \subsection{Avec du \og text-postprocessing \fg}
            \label{text_postprocessing}

                \subsubsection{Le principe}
                Afin de pallier ces problèmes de mise en forme de texte, on assouplit un peu les contraintes par rapport à l'égalité stricte.
                En effectuant un traitement de text processing, à la fois sur la ground truth et les résultats du modèle, on va comparer des textes un peu plus \og standardisés \fg.
                Les traitements effectués sont les suivants :
                \begin{itemize}
                    \item On passe le texte en minuscules
                    \item On retire la ponctuation
                    \item On remplace tous les \og whitespaces \fg (retours à la ligne, espaces multiples, tabulations, \dots) par des espaces simples
                    \item On retire les accents
                \end{itemize}
                L'ensemble de ces transformations sont faites en utilisant les fonctionnalités proposées par le CountVectorizer de la bibliothèque scikit-learn (cf. le notebook en annexe \mref{code:performance_measurement} et le module pimest inclus en annexe \mref{code:pimest}).

                \subsubsection{Les résultats}

                Si on évalue cette nouvelle accuracy avec le text processing, sur l'échantillon d'entraînement après entraînement sur l'échantillon d'entraînement, on obtient une accuracy de 14\% (14 listes d'ingrédients correctement prédites sur 100).
                Ces 14 listes d'ingrédients sont présentées à la \reftable{tbl:GT_postprocessed_corrects}.

               {\renewcommand{\arraystretch}{1.5}%
                \begin{table}
                    \begin{spacing}{1.0}
                    \begin{center}
                    {\scriptsize
                    \input{tbls/GT_postprocessed_corrects.tex}
                    }
                    \caption{Prédictions identifiées comme correctes après postprocessing}
                    \label{tbl:GT_postprocessed_corrects}
                    \end{center}
                    \end{spacing}
                \end{table}
                }

                De la même manière que précédemment, si on fait une cross-validation sur l'ensemble des données manuellement étiquetées, on obtient une accuracy de $16.60\% \pm 3.35\%$.

                \subsubsection{Les limites de cette métrique}

                Cette métrique est déjà plus intéressante que l'approche naïve, mais elle a quand même un défaut majeur : elle a une vision encore trop binaire des résultats.
                En effet, que le texte soit identique à un préfixe près (cf. la quatrième ligne de la \reftable{tbl:GT_prediction_sample}), ou qu'il n'ait rien à voir (d'autres exemples sont présents dans cette même table), elle considèrera la prédiction comme erronée.
                Or, il est important d'identifier les cas où le modèle s'est complètement trompé par rapport à ceux où il a quand même identifié le bon bloc contenant les ingrédients.

        \section{Fonctions de \og similarité \fg spécifiques}

        On peut définir des fonctions de similarité, qui permettent d'être plus fin qu'une simple évaluation binaire du résultat du modèle.
        Cela permet de prendre plus finement en compte les cas où le modèle a identifié un bloc très similaire à la ground truth.
        On va pour cela s'appuyer sur diverses métriques permettant de mesurer l'écart entre des chaînes de caractères.
        \`{A} chaque fois, on définira une fonction de scoring qui sera une similarité.
        De plus, elles seront normées pour prendre leurs valeurs entre 0 (chaînes de caractères totalement différentes) et 1 (chaînes identiques), ce qui permettra d'exprimer ces métriques en pourcentage.
        Ces similarités seront calculées après application du text-postprocessing, comme décrit à la section \mref{text_postprocessing}.
        L'ensemble de ces similarités seront calculées sur les caractères.

            \subsection{Similarité basée sur la distance de Levenshtein}

            La distance de Levenshtein~\cite{levenshtein_wiki}  entre deux chaînes de caractères est la distance d'édition pour passer de l'une à l'autre en prenant en compte les transformations suivantes :
            \begin{itemize}
                \item insertion d'un caractère 
                \item suppression d'un caractère
                \item substitution d'un caractère par un autre
            \end{itemize}
            Cette distance possède les caractéristiques suivantes : 
            \begin{itemize}
                \item elle peut se calculer entre deux chaînes de longueur différentees
                \item elle a pour minimum 0 (si et seulement si les deux chaines sont identiques)
                \item a pour majorant la longueur de la plus longue des deux châines
            \end{itemize}
            Pour construire une fonction de similarité telle que présentée en introduction de cette section, on appliquera la transformation suivante : 
            \[sim_{lev}(s_{1}, s_{2}) = 1 - \frac{dist_{lev}(s_{1}, s_{2})}{max(len(s_{1}), len(s_{2}))}\]
            en notant $dist_{lev}$ la distance de Levenshtein, $len(s)$ la longueur de la chaîne $s$, et $s_{1}$ et $s_{2}$ les chaînes de caractères à comparer.
            Par exemple, la distance entre les chaînes \og rateaux \fg et \og chameau \fg vaut 4 :
            \begin{itemize}
                \item substitution du \og r \fg par un \og c \fg
                \item insertion du \og h \fg
                \item substitution du \og t \fg par un \og m \fg
                \item suppression du \og x \fg
            \end{itemize}

            \subsection{Similarité basée sur la distance de Damerau-Levenshtein}

            La distance de Damerau-Levenshtein~\cite{damerau_levenshtein_wiki} est une variante de la distance de Levenshtein.
            Elle calcule également une distance d'édition, mais en ajoutant une transformation possible : l'interversion de deux caractères successifs.
            Les transformations possibles pour cette transformation sont :
            \begin{itemize}
                \item insertion d'un caractère 
                \item suppression d'un caractère
                \item substitution d'un caractère par un autre
                \item interversion de deux caractères successifs
            \end{itemize}
            Ses caractéristiqes sont les mêmes que la distance de Levenshtein (minimum, maximum) ; et elle est toujours inférieure ou égale à la distance de Levenshtein.
            On convertit cette distance en similarité de la même manière que la distance de Levenshtein.

            \subsection{Similarité de Jaro}
            
            La similarité de Jaro~\cite{jaro_wiki} est une fonction permettant de mesurer une similarité entre 0 (chaînes complètement différentes) et 1 (chaînes parfaitement identiques).
            L'heuristique derrière cette similarité est de considérer qu'entre deux chaînes, un caractère est \og correspondant \fg (\og matching \fg) s'il est déplacé de moins de la moitié de la longueur de la plus longue des deux chaînes.
            Ensuite, la similarité est fonction croissante du nombre de caractères \og correspondants \fg et décroissante du nombre de caractères \og correspondants \fg mais mal placés.
            Sur des chaînes longues de quelques dizaines de caractères, la quasi-totalité des caractères seront \og correspondants \fg.
            Elle est en général plutôt adaptée à des comparaison de chaînes courtes telles que des noms propres ou des mots de passe.

            \subsection{Similarité de Jaro-Wrinkler}
            
            La similarité de Jaro-Winkler~\cite{jaro_winkler_wiki} est une variante de la similarité de Jaro.
            Elle possède la même heuristique que la distance de Jaro, avec en plus comme caractéristique de donner un poids plus important aux 4 caractères formant le début de la chaîne de caractères.
            Cette métrique est encore moins adaptée au cas d'usage que la précédente.

            \subsection{\'{E}valuation de ces similarités sur la ground truth}

            Les résultats de l'évaluation de chacune de ces similarités sont présentés à la \reftable{tbl:similarities_result}.
            \begin{table}[htbp]
                \begin{center}
                \input{tbls/similarities_result.tex}
                \caption{\'{E}valuation du modèle en utilisant les métriques de similarité}
                \label{tbl:similarities_result}
                \end{center}
            \end{table}
            On constate que :
            \begin{itemize}
                \item les similarités de Levenshtein et Damerau-Levenshtein donnent des résultats identiques
                \item les similarités de Jaro et de Jaro-Winkler donnent des évaluations très généreuses de la performance du modèle, comme on pouvait s'y attendre sur la base de textes longs
            \end{itemize}

            \subsection{Décision sur la métrique à utiliser et illustration}
            
            Les similarités de Jaro et Jaro-Winkler sont abandonnées car non pertinentes pour notre cas d'usage, qui se base sur des textes de plusieurs dizaines de caractères.
            Les similarités de Levenshtein et Damerau-Levenshtein donnant des résultats identiques, on choisit la distance de Levenshtein car son implémentation en C (bibliothèque python-Levenshtein) semble plus performante que celle en C++ de la bibliothèque Jellyfish.

            Des exemples du début, du milieu et du bas du classement en termes de similarité de Levenshtein sur le jeu de test, après entraînement sur le jeu d'entraînement, sont présentés à la \reftable{tbl:similarity_illustration}.
            \begin{table}[htbp]
                \begin{spacing}{1.0}
                \begin{center}
                {\tiny
                \input{tbls/similarity_illustration.tex}
                }
                \caption{Illustration de l'évaluation du modèle à l'aide des métriques de similarité}
                \label{tbl:similarity_illustration}
                \end{center}
                \end{spacing}
            \end{table}

        \subsection{Performance en fonction de la longueur de la liste d'ingrédients}

        \`{A} la section \mref{prediction_gt_illustration}, on avait mentionné le fait que le modèle semblait moins performant lorsque la liste d'ingrédients cible était courte.
        Maintenant que nous avons une fonctionnalité de mesure de la performance, il est possible de confirmer ou d'infirmer ce point.
        La représentation de ce lien est présentée en \reffig{fig:perf_vs_length}.
        Il y a bien une corrélation positive entre longueur de la liste d'ingrédients et performance du modèle ($r^{2} \approx 0.340$, après retrait des outliers).
        Le mode de production de ce graphe est présenté à la fin du notebook \og Mesure de la performance \fg inclus en annexe \mref{code:performance_measurement}.
        \begin{figure}[htbp]
            \begin{center}
            \includegraphics[width=0.7\linewidth]{img/perf_vs_length.png}
            \end{center}
            \caption{Performance du modèle en fonction de la longueur des listes d'ingrédients}
            \label{fig:perf_vs_length}
        \end{figure}

    \chapter{Ajustement des paramètres}
    \label{model_tuning}

        \section{Description des paramètres ajustables}

        Les paramètres ajustables sur le modèle présenté ci-dessus sont multiples (text-preprocessing, découpage des textes en blocs, calcul de similarité, \dots).
        Maintenant qu'un pipeline équipé d'une fonctionnalité de mesure de la performance est disponible, on va pouvoir faire varier les paramètres et évaluer l'impact sur la performance du modèle.

        \subsection{Text-preprocessing}

        L'étape de text-preprocessing vise à nettoyer et standardiser le contenu des textes.
        Elle est appliquée aux textes avant entraînement et avant prédiction.
        Il est nécessaire de bien appliquer les mêmes règles à ces deux étapes.
        Parmi les fonctionnalités classiques de texte préprocessing, on va procéder de la manière suivante : 

        \emph{Mise en minuscule} : 
        On appliquera systèmatiquement la mise en minuscule. 
        La casse est plutôt un facteur qui va pénaliser l'interprétation du texte, en contribuant à la création artificielle de mots synonymes.

        \emph{Retrait des accents} : 
        L'impact du retrait des accents sera mesuré sur la performance du modèle.
        En effet, même s'il peut amener à la création d'homonymes, sur des textes en français inclus dans des pdf et issus de multiples sources cette fonctionnalité peut avoir une influence positive.

        \emph{Gestion des stopwords} : 
        On vérifiera l'impact de retirer les stopwords (mots fréquents peu porteurs de sens) de l'ensemble des textes (listes d'ingrédients cible, contenu des documents) avant entraînement ou prédiction.
        On se basera sur une liste déterminée à partir des mots les plus fréquents dans le corpus des documents, qui ne portent pas de signification.
        Il est pressenti que ces stopwords risquent d'avoir un impact important sur le résultat de l'algorithme, en ajoutant dans l'ensemble des blocs candidats un grand nombre de mots étant considérés comme des ingrédients.

        \subsection{Découpage du texte des documents en blocs}
        \label{splitting_functions}

        On peut envisager d'autres méthodes de découpage des textes complets des documents en blocs candidats.
        Il avait été vu lors du prototypage que cette fonction pouvait influer fortement sur la qualité du résultat (il est inutile de chercher une bonne solution pour trouver le bon candidat si on détermine mal la liste de candidats).
        On éprouvera les techniques suivantes : 
        \begin{itemize}
            \item comme en prototypage, en coupant dès que deux retours à la lignes successifs sont identifiés
            \item à chaque retour à la ligne
            \item via une expression régulière, on coupera dès qu'on trouve un ensemble de whitespaces (espaces, retour à la ligne, tabulation, \dots) contenant au moins 2 retours à la ligne
        \end{itemize}
        
        \subsection{Vectorisation des textes}
        \label{vectorisation}

        \emph{Bag Of Words} : 
        Les textes seront représentées sous forme de vecteurs via l'approche \og Bag Of Words \fg.
        On évaluera les méthodes suivantes pour la transformation des textes en vecteurs :
        \begin{itemize}
            \item de manière binaire : la coordonnée est à 1 si le mot est présent au moins une fois dans le texte, 0 sinon
            \item en term-frequency (tf) : la coordonnée correspond à la fréquence d'apparition du mot dans le texte
            \item en tf-idf, l'inverse document frequency pouvant être calculée en référence au corpus du contenu des documents
            \item via une fonction de score absolu, tel que décrit dans la partie sur l'analyse des données, à la section \mref{word_scores}
            \item via une fonction de score relatif (décrit dans la même section)
        \end{itemize}
        
        \emph{n-grams} : 
        On testera également la possibilité de prendre en compte des n-grams de mots (mots successifs) et les résultats que cela amène (à la fois dans l'entraînement et dans la prédiction).

        \emph{Embeddings des mots} : 
        Enfin, on procédera aux calculs d'embeddings de mots, en appliquant les méthodes suivantes :
        \begin{itemize}
            \item truncated SVD~\cite{LSA_wiki} : on approxime la matrice des textes vectorisés en calculant ses valeurs singulières et en n'en gardant que les 100 plus importantes (valeur choisie arbitrairement). Les embeddings des mots sont récupérable directements depuis le transformateur TruncatedSVD mis à disposition dans la bibliothèque scikit-learn.
            \item Word2Vec~\cite{word2vec_wiki} : cette méthode de calcul des embeddings se base sur l'entrainement d'un réseau de neurones pour la prédiction d'un mot à partir des autres mots de son contexte (Continuous Bag Of Words, CBOW), ou de prédire le contexte à partir d'un mot (skip-gram). Les embeddings sont les poids de la couche intermédiaire du réseau de neurones après entraînement. On utilisera l'algorithme CBOW, avec un nombre de features de 100.
        \end{itemize}
        Cette manière de représenter les textes et les mots se traduit par une \emph{réduction de la dimensionalité}, qui passe du nombre de mots dans le vocabulaire (plus de 10 000 dans notre cas) au nombre de features conservées (on en gardera 100).

        \subsection{Calcul de similarité}

            \subsubsection{Similarité cosinus}
            \label{similarite_cosinus}

            Cette méthode nécessite de représenter le vocabulaire des ingrédients sous la forme d'un vecteur, en faisant la moyenne des vecteurs de textes obtenus lors de la vectorisation des listes d'ingrédients.
            On calcule le cosinus de l'angle entre le vecteur du vocabulaire et celui du candidat.
            Plus ce cosinus est élevé, plus la similarité est importante.
            Cette similarité n'a pas de paramètre, une fois que les vecteurs ont été calculés.
            Il est possible d'appliquer ce calcul de similarité y compris après application d'un embedding des vecteurs de texte (ex: Word2Vec, tSVD, \dots).
            Une illustration de la similarité cosinus est proposée à la \reffig{fig:similarite_cosinus}.
            Enfin, ce mode de calcul de la similarité permet d'ajuster la direction du vecteur cible en fonction de divers critères (fonctions de scoring spécifiques, cf. section \mref{vectorisation} sur la vectorisation des textes).
            La similarité cosinus est toujours comprise entre 0 (vecteurs orthogonaux) et 1 (vecteurs colinéaires), sauf dans le cas où on applique des coefficients négatifs à certains mots (cas de la fonction de scoring relatif, où des poids sont négatifs).

            \subsubsection{Par projection}
            \label{similarite_projection}

            Cette méthode nécessite uniquement de projeter le candidat sur le sous-espace généré par les mots du vocabulaire des ingrédients.
            On fait ensuite le rapport entre la norme du vecteur initial, à celle de sa projection. 
            On peut envisager cette similarité un peu comme un calcul de l'angle entre le vecteur candidat et le sous-espace cible.
            Attention, si on utilise la même norme pour mesurer le vecteur initial et sa projection, on choisira toujours les candidats dont aucun mot n'est hors du vocabulaire (cf. l'illustration présentée à la \reffig{fig:similarite_projection}).
            Une manière de s'affranchir de ceci est choisir pour mesurer le vecteur initial une norme $L_{p}$ et pour sa projection une norme $L_{q}$ avec $p > q$.
            En effet, un résultat d'algèbre linéaire~\cite{lpnorms} montre que :
            \[\forall x \in \mathbb{R}^{n}, \; p > q \Rightarrow \lVert x \rVert_{p} \leqslant \lVert x \rVert_{q}\]
            Pour rappel, la norme $L_{p}$ d'un vecteur $x$ est définie par : 
            \[\lVert x \rVert_{p} = \left( \sum_{i} |x_{i}|^{p} \right)^{1/p} \]
            où les $x_{i}$ sont les coordonnées de $x$.
            Ce calcul de similarité ne peut se faire que dans l'espace initial, où les vecteurs représentant les textes peuvent être projetés sur un espace généré par des mots.
            Si une réduction de la dimensionalité a eu lieu, par exemple via le calcul d'embeddings, il n'est pas possible de déterminer un sous-espace généré par les mots du vocabulaire des ingrédients.
            De plus, contrairement au calcul de similarité cosinus, il n'est pas possible de pondérer les différents mots du vocabulaire des ingrédients (via un calcul de score spécifique, un tf-idf, \dots).
            Ce mode de calcul est paramétrique, c'est à dire qu'il faut définir la norme de l'espace de départ et la norme dans le sous-espace de projection.
            Contrairement à la similarité cosinus, la similarité projection peut être supérieure à 1.

            Le mode de calcul retenu dans les prototypes décrits précédemment (chapitre \mref{prototypes}) est un calcul de similarité par projection utilisant les normes $L_{2}$ sur l'espace initial et $L_{1}$ sur le sous-espace de projection.
            La formule de calcul était : 
            \[\frac{\text{Nombre de mots du candidat qui sont des ingrédients}}{\text{Norme euclidienne du vecteur du candidat}} = \frac{\lVert\text{Vecteur projeté}\rVert_{1}}{\lVert\text{Vecteur initial}\rVert_{2}}\]
            où le vecteur initial est la matrice des textes avec les comptes de mots.

            \subsubsection{Illustration de ces calculs de similarité}

            On illustre ces deux manières de mesurer la similarité en prenant l'exemple suivant.
            Le vocabulaire des ingrédients est constitué des mots \og eau \fg et \og sucre \fg, et le mot \og eau \fg est 2 fois plus représenté que le mot \og sucre \fg.
            Les textes candidats sont \og sucre eau sucre \fg et \og commercial commercial sucre \fg.
            Ces deux exemples sont présentés en \reffig{fig:similarite_cosinus} et \reffig{fig:similarite_projection}.
            Sur ces figures, la similarité correspond aux angles matérialisés en vert.
            Si on récapitule les valeurs respectives des similarités, on obtient les résultats présentés à la \reftable{tbl:similarites_exemples}
            
            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.5\linewidth]{img/similarite_cosinus.png}
                \end{center}
                \caption{Illustration de la similarité cosinus}
                \label{fig:similarite_cosinus}
            \end{figure}     

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.6\linewidth]{img/similarite_projection.png}
                \end{center}
                \caption{Illustration de la similarité projection}
                \label{fig:similarite_projection}
            \end{figure}     

            \begin{table}[hbtp]
            \begin{center}
            \begin{small}
            \begin{tabular}{lcccc}
                \toprule
                \textbf{Candidats} &  \textbf{Cosinus} &  \textbf{Projection $L_{1}/L_{1}$} & \textbf{Projection $L_{2}/L_{1}$} & \textbf{Projection $L_{3}/L_{1}$}\\
                \midrule
                \textbf{sucre eau sucre} &  0,8 & 1, & 1,34 & 1,44 \\
                \textbf{commercial commercial sucre} & 0,2 & 0,33 & 0,45 & 0,48 \\
                \bottomrule
            \end{tabular}
            \end{small}
            \caption{Exemples de calculs de similarité}
            \label{tbl:similarites_exemples}
            \end{center}
            \end{table}

    \section{Ajustements et résultats}
            
    On peut, dans l'optique d'améliorer la performance du modèle, ajuster certains paramètres et d'évaluer l'impact via une grid search.
    Pour l'ensemble des ajustements, on travaillera uniquement sur le set d'entrainement (400 échantillons) afin de se prémunir des effets de data leaking (qui amènerait à surestimer la performance lors de l'évaluation finale).
    On travaillera avec 8 folds de cross-validation, afin de rester sur des ordres de grandeurs comparables à ce qui avait été fait lors de l'évaluation du second prototype (10 folds sur 500 échantillons).
    Enfin, on effectuera une évaluation finale sur le set de test, après entraînement du modèle avec les meilleures performances sur le set d'entraînement.
    La sélection des meilleurs paramètres se fera sur la base de la similarité de Levenshtein (cf. chapitre \mref{performance}), mais on illustrera également la performance du modèle retenu via l'accuracy avec text-postprocessing.
    L'ensemble des résultats présentés à cette section sont issus du notebook \og Tuning du modèle \fg présenté en annexe \mref{code:model_tuning}.

        \subsection{Ajustement du text-preprocessing}

            Les résulats de la grid search sur le text-preprocessing sont présentés à la \reffig{fig:tuning_prepro}.
            Dans l'ensemble des représentations de ce chapitre, chaque point du swarmplot représente une cross validation de l'ensemble du set de test.
            Les folds individuels ne sont pas représentés, l'écart type pour un jeu de paramètres donné n'est donc pas représenté (le détail est présenté dans le notebook mentionné précédemment).
            Dans ce premier run, les paramètres explorés sont : 
            \begin{description}
                \item[gestion des accents :] conservation ou retrait des accents sur les textes
                \item[gestion des stopwords :] conservation ou retrait des stopwords ('pas', 'le', 'en', 'pour', 'ou', 'ce', 'de', 'dans', 'du', 'and', 'un', 'sur', 'et', 'of', 'est', 'par', 'la', 'les', 'dont', 'au', 'des', 'que') des textes
                \item[constitution des ngrams :] seulement les monogrammes, puis monogrammes et bigrammes, puis monogrammes et bigrammes et trigrammes
                \item[découpage des textes en blocs :] comparatif des trois fonction présentées précédemment à la section \mref{splitting_functions}
                \item[fonctions de similarité :] identification du meilleur candidat par similarité cosinus, ou par projection $L_{2}/L_{1}$
            \end{description}
            soit un total de 72 jeux de paramètres testés.
            Il apparaît que :
            \begin{itemize}
                \item le retrait des stopwords a un impact fortement positif sur la performance de l'algorithme
                \item le retrait des accents a un impact positif mais relativement marginal
                \item l'écart-type sur la performance (similarité de Levenshtein) est très élevé au sein d'un jeu de paramètre testé : il se situe systématiquement aux alentours de 5-6\% pour une valeur moyenne autour de 50\%
            \end{itemize}

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.9\linewidth]{img/tuning_prepro.png}
                \end{center}
                \caption{Résultats du tuning du preprocessing}
                \label{fig:tuning_prepro}
            \end{figure}     

            \subsection{Tuning du découpage du texte des documents}

            On se sert des résultats de la grid search présentée précédemment.
            Les résultats sont présentés à la \reffig{fig:tuning_split}.
            On en déduit que la fonction qui présente les meilleurs résultats est la troisième fonction (via une expression régulière avec 2 retours à la ligne).
            L'écart n'est pas énorme avec la fonction 1 (1 à 2\% sur l'accuracy moyenne), mais il est présent de manière consistante.

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.9\linewidth]{img/tuning_split.png}
                \end{center}
                \caption{Résultats du tuning du découpage des textes des documents}
                \label{fig:tuning_split}
            \end{figure}

            \subsection{Comparatif des fonctions de similarité}

            Dans une seconde grid search, on va comparer les similarité cosinus et projection, en faisant varier les paramètres de la similarité projection (normes de l'espace complet et du sous-espace de projection).
            Les résultats sont présentés à la \reffig{fig:tuning_similarite}.
            Ils sont issus d'une grid search lors de laquelle on a fixé : 
            \begin{itemize}
                \item la tokenisation avec des monogrammes et des bigrammes
                \item le retrait des accents
                \item le retrait des stopwords
                \item la fonction de découpage en blocs numéro 3 (avec l'expression régulière)
            \end{itemize}
            et on a fait varier : 
            \begin{itemize}
                \item les fonctions de similarité, cosinus ou projection
                \item les normes utilisées, pour les fonctions de projection
                \item la construction de la matrice des textes entre les comptes de mots, ou l'identification uniquement de la présence / absence d'un terme (\og binary Bag Of Word \fg)
            \end{itemize}
            Dans la représentation qui en a été faite, on a regroupé ensemble les similarité projection en fonction de l'écart de valeur entre les normes dans l'espace initial et le sous-espace de projection (ex : pour la similarité projection $L_{3}/L_{1}$ cet écart vaut 2);
            En peut en tirer des conclusions intéressantes : 
            \begin{itemize}
                \item les écarts-types au sein d'une cross-validation sont toujours importants (5-6\% environ, sur la similarité de Levenshtein)
                \item la similarité par projection surperforme nettement la similarité cosinus
                \item comme on pouvait s'y attendre, les performances sont catastrophiques si on applique pour la similarité projection la même norme dans l'espace de départ et le sous-espace de projection (on choisira toujours un texte qui n'a aucun mot hors du vocabulaire des ingrédients, peu importe sa longueur)
                \item des tendances régulières se dessinent parmi les groupes avec un delta constant, bien visibles sur l'indicateur de performance en similarité : 
                \begin{itemize}
                    \item les performances semblent suivre deux tendances polynomiales avec des coefficient d'ordre maximal négatif, différentes selon que la tokenisation est binaire ou non
                    \item la tokenisation binaire est systématiquement meilleure que la tokenisation par comptes
                    \item on atteint un maximum de performance aux alentours des projections $L_{4}/L_{3}$, $L_{6}/L_{4}$, $L_{8}/L_{5}$
                \end{itemize}
            \end{itemize}

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.9\linewidth]{img/tuning_similarite.png}
                \end{center}
                \caption{Résultats du tuning des fonctions de similarité}
                \label{fig:tuning_similarite}
            \end{figure}

            \subsection{Comparaison des méthodes de vectorisation}

            \subsubsection{Inverse document frequency}

            On peut apprécier l'impact de la prise en compte de l'inverse document frequency lors de la vectorisation, sur la performance du modèle.
            Cette inverse document frequency se calcule par rapport à la présence des termes dans le corpus des textes des documents.
            Les résultats sont présentés à la \reffig{fig:tuning_idf}, sur la base d'une grid search faisant varier : 
            \begin{itemize}
                \item le type de similarité : cosinus ou projection $L_{4}/L_{3}$
                \item l'utilisation ou non de l'inverse document frequency
                \item la prise en compte de ngrams, avec des ngrams de longueur de 1 à 5
            \end{itemize}
            L'impact est très différent selon qu'on identifie le meilleur candidat via le cosinus, plutôt que via la projection.
            La qualité des prédictions s'améliore en utilisant l'inverse document frequency couplée à la similarité cosinus, mais est très fortement dégradée dans le cas de la projection.

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.9\linewidth]{img/tuning_idf.png}
                \end{center}
                \caption{Résultats du tuning de l'utilisation de l'inverse document frequency}
                \label{fig:tuning_idf}
            \end{figure}

            \subsubsection{Prise en compte des n-grams}

            La vectorisation des textes peut également être ajustée, en ajoutant en tant que features des ngrams de longueur variable.
            L'impact de la prise en compte de ngrams de plus en plus long (de 1 à 5 mots) est illustré à la \reffig{fig:tuning_ngrams}.

            On peut constater que :
            \begin{itemize}
                \item la prise en compte de ces ngrams a des effets mitigés dans le cas du cosinus
                \item elle est par contre bénéfique pour la projection, avec une saturation à partir des trigrammes
            \end{itemize}

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.9\linewidth]{img/tuning_ngrams.png}
                \end{center}
                \caption{Résultats du tuning de l'utilisation de l'inverse document frequency}
                \label{fig:tuning_ngrams}
            \end{figure}

            \subsubsection{Fonctions de scoring spécifique}

            Les fonctions de scoring \og absolu \fg et \og relatif \fg ont été présentée dans le chapitre sur l'analyse des données, à la section \mref{word_scores}.
            Elles permettent de calculer un score pour chaque mot, afin de quantifier son \og affinité \fg pour les listes d'ingrédients.
            L'utilisation d'un score relatif, qui peut être négatif (i.e. le mot concerné est plus souvent présent dans le corps du texte des documents que dans les listes d'ingrédients), et donc pénaliser le candidat. 
            C'est par exemple le cas du mot \og sel \fg, qui est évidemment un ingrédient mais qui est également très représenté par ailleurs dans le contenu des fiches techniques.
            L'utilisation de ces scores ne se peut se faire que pour la similarité cosinus, de la même manière qu'illustré à la \reffig{fig:similarite_score} (qui montre la \og négativité \fg du score du mot \og sel \fg).
            Il est même possible que la similarité du candidat avec le vecteur cible soit négative.

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.6\linewidth]{img/similarite_score.png}
                \end{center}
                \caption{Illustration de la similarité cosinus avec score relatif}
                \label{fig:similarite_score}
            \end{figure}

            L'impact des fonctions de scoring a été illustré sur la base d'une grid search s'appliquant uniquement au mode d'identification cosinus, sans n-grams, avec retrait des stopwords et des accents.
            Elle fait varier :
            \begin{itemize}
                \item la comptabilisation des mots de manière binaire ou via les comptes
                \item la détermination ou non de l'inverse document frequency
                \item les scores : sans, avec le score absolu ou avec le score relatif
                \item les embeddings de mots : sans, Word2Vec, truncated SVD
            \end{itemize}
            Les résultats sont présentés à la \reffig{fig:tuning_score}.
            On constate que : 
            \begin{itemize}
                \item l'utilisation du score relatif dégrade très fortement la performance du modèle
                \item l'utilisation de l'idf avec le score absolu dégrade légèrement la performance
                \item le seul impact positif à l'utilisation d'une fonction de scoring est que score absolu améliore marginalement la performance en l'absence d'idf
            \end{itemize}
            La cause de la très faible performance du score relatif semble être la suivante : comme illustré précédemment à la \reffig{fig:relative_score_bar}, du fait de l'écart entre la taille des vocabulaires ingrédients et documents, les scores relatifs sont en moyenne négatifs.
            Par conséquent, plus ils sont longs, plus les vecteurs candidats ont en moyenne un produit scalaire négatif avec le vecteur cible.
            Ce qui signifie que le choix du meilleur candidat revient à prendre en compte le vecteur le moins négatif, voire un bloc de texte vide ou constitué de mots inconnus à l'entrainement\dots            

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.6\linewidth]{img/tuning_score.png}
                \end{center}
                \caption{Illustration de la similarité cosinus avec score relatif}
                \label{fig:tuning_score}
            \end{figure}

            \subsection{Utilisation d'embeddings}

            L'utilisation d'embeddings a été évaluée via la même grid search que pour l'évaluation du scoring.
            Les résultats sont illustrés à la \reffig{fig:tuning_embedding}.
            On constate que : 
            \begin{itemize}
                \item les embeddings issus de la truncated SVD sous-performent par rapport aux autres méthodes (sans embeddings ou Word2vec)
                \item l'utilisation de Word2vec n'augmente globalement pas la performance du modèle
            \end{itemize}
            L'utilisation de ces embeddings, entraînés sur notre jeu de données uniquement, n'est pas concluant.

            \begin{figure}[htbp]
                \begin{center}
                \includegraphics[width=0.6\linewidth]{img/tuning_embedding.png}
                \end{center}
                \caption{Illustration de la similarité cosinus avec score relatif}
                \label{fig:tuning_embedding}
            \end{figure}

            \section{\'{E}valuation finale de la performance}

            On peut tirer les enseignements suivants des travaux menés :
            \begin{itemize}
                \item des gains rapides on été obtenus en travaillant sur le preprocessing de base : stopwords, gestion des accents, découpage en blocs
                \item la similarité projection sur-performe nettement la similarité cosinus
                \item l'utilisations d'algorithmes plus complexes (embeddings et scoring) ne suffit pas à compenser cet écart de performance
            \end{itemize}

            Les paramètres ayant obtenu la meilleure performance ($63.31\% \pm 3.83\%$) sur la grid search sont : 
            \begin{itemize}
                \item similarité projection, avec les normes $L_{4}/L_{3}$
                \item retrait des stopwords, des accents
                \item découpage en blocs via la fonction 3 (avec expression régulière)
                \item vectorisation binaire des textes, sans appliquer l'inverse document frequency
                \item prise en compte des monogrammes, bigrammes et trigrammes
            \end{itemize}

            Si on entraîne un modèle avec ces paramètres sur le set d'entrainement, et qu'on évalue sa performance sur le set de test (qui n'a pas été utilisé lors du tuning des paramètres), on obtient les résultats suivants : 
            \begin{itemize}
                \item Similarité de Levenshtein moyenne : $67.18\%$
                \item Accuracy postprocessée : $27\%$
            \end{itemize}
            Ces résultats sont globalement bons, et illustrés en annexe à la \reftable{tbl:final_prediction}.
            Pour rappel, la performance initiale d'un modèle avant ajustement des paramètres était de $48.86\%$ (cf. \reftable{tbl:similarities_result}).
            Il apparaît que les principales améliorations passeraient par la mise en place de fonctions de découpage plus précises, de fonctionnalités de text postprocessing pour éliminer les préfixes, et d'OCR pour prendre en compte les documents numérisés.
