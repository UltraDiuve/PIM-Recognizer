\part{Construction du modèle}
    \chapter{Les principes généraux}
        \section{Contenu du texte d'une liste d'ingrédients}

        En général, chaque ingrédient sera présent une seule fois dans la liste (cf. section \mref{listes_ingredients})

        Le calcul d'embeddings via des modèles tels que SVD ou Word2Vec fait peu de sens.
        \newline
        \newline
        \emphbox{l'extraction des textes se fait au format \emph{Bag Of Words}, sans utiliser de notion d'IDF. L'utilsation de TF semble églament ne pas amener de valeur à priori.}

        \section{Limitation à l'identification des listes d'ingrédients}

        On est sur une taxonomie d'informations limitée dans les fiches techniques.

        On pourrait envisager de classifier l'ensemble des textes présents dans les fiches techniques.

        Mais l'absence de données étiquetées rend cette tâche impossible. La charge d'étiquetage d'un nombre représentatif de blocs de texte de fiches techniques est trop importante pour être mise en oeuvre dans le cadre de ce projet.

        \section{Conversion de documents en texte}
        
        dire ici qu'on utilise principalement pdfminer vs. d'autres outils d'OCR.

        De plus, on partira dans un premier temps sur une transformation basique d'un document en texte, sans passer par une analyse de la localisation des textes sur le document (cf. les difficultés présentées dans la section \mref{formats_spatialisation}).
            
    \chapter{Construction d'un modèle simple \og ouvert \fg}
        
    Le fonctionnement global de ce premier modèle (présenté à la \reffig{fig:open_model}) ne respecte pas les principes du Machine Learning.
    Il permet juste d'éprouver la méthode préssentie, ainsi que de se faire une idée de l'efficacité d'un modèle de ce type.
    En effet, même si on utilise des briques d'extraction de features depuis des textes, il manque une partie de mesure de la performance, indispensable pour pouvoir évaluer et améliorer la pertinence du modèle.
    On peut toutefois tout de même effectuer un train/test split, afin d'éviter de surestimer la valeur des résultats.    
    Le code est présenté en annexe (ref à mettre), il utilise les classes IngredientExtractor et PIMIngredientExtractor.
    Il n'utilise pas les données étiquetées manuellement (présentées à la section \mref{manually_labelled_data}).

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=0.9\linewidth]{img/open_model.png}
        \end{center}
        \caption{Schéma de principe du \og modèle ouvert \fg}
        \label{fig:open_model}
    \end{figure}     


        \section{Extraction des données}

        Ne garder que produits d'épicerie et boissons non alcoolisées

        \section{Conversion en blocs de texte}

        On utilise la bibliothèque PDFMiner.six. 
        Elle nous sort un long string qui contient le texte entier du document.
        On applique une \og bête \fg fonction : on splitte ce string quand on observe 2 retours à la lignes consécutifs.
        Le code est présenté en annexe.
        Pour le moment, vu la proportion importante de PDF dont le contenu est extractible

        \section{Train/Test split}

        On fait un split 50/50, on se base sur les uid pour identifier les produits.
        Sur le train set, on récupère les listes d'ingrédients du PIM.

        \section{Entrainement du modèle}

        L'entraînement est basique : on constitue seulement un vocabulaire en utilisant la fonctionnalité mise à disposition dans scikit-learn.

        \section{Calcul de la similarité}

        On calcule la similarité cosinus entre chacun des blocs, et le vocabulaire.
        On prend, systématiquement l'argmax de la similarité qu'on propose comme liste d'ingrédients.

        \section{Illustration des résultats obtenus}
    
        Mettre ici les résultats sur quelques fiches techniques présentées en annexe.
        Spoiler : rien que comme ça, les résultats sont encourageants.

        \section{Pistes d'améliorations identifiées}

        En plus de la mesure de la performance, qui est indispensable avant de pouvoir procéder à des ajustements.
        Pistes identifiées : 
        \begin{itemize}
            \item Faire un découpage du gros texte en blocs plus malin, potentiellement avec des expressions régulières
            \item Faire des \og ngrams de blocs \og, ce qui permettrait de parfois fusionner des blocs qui ont été séparés (car contenaient des retours à la ligne successifs)
            \item Essayer une autre manière de calculer la similarité ?
        \end{itemize}

    \chapter{Utilisation des données manuellement étiquetées}

    Comme présenté à la section\mref{ingredient_comparison} relative à la comparaison entre les données du PIM et celles récupérées lors de l'étiquetage, il y a un grand nombre d'écarts.
    Or, si on entraîne le modèle et qu'on mesure sa performance sur des données de mauvaise qualité, on aura de mauvais résultats.
    On va donc construire un modèle se basant sur les données manuellement étiquetées.
    Le fonctionnement de ce modèle est présentés à la \reffig{fig:ground_truth_model}.
    La méthodologie utilisée à cette partie est présentée dans le notebook \og Modèle basé sur les données manuellement étiquetées \fg en annexe \mref{code:gt_based_model}.
    Les différents transformateurs et estimateurs spécifiques sont définis dans le module pimest, inclu en annexe \mref{code:pimest}.
    
    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=0.9\linewidth]{img/ground_truth_model.png}
        \end{center}
        \caption{Schéma de principe du modèle basé sur les données étiquetées}
        \label{fig:ground_truth_model}
    \end{figure}     

        \section{Chargement des données manuellement étiquetées}

        La toute première étape est la constution d'un dataframe contenant : 
        \begin{itemize}
            \item les uid pour indexer les produits
            \item les listes d'ingrédients manuellement étiquetées depuis les fiches techniques
            \item le contenu de chacune des fiches techniques au format texte
        \end{itemize}
        On commence par charger les données du fichier csv contenant les uid et les listes d'ingrédients.
        Ensuite, un pipeline scikit learn d'acquisition des données est lancé.
        Il s'agit de 3 transformateurs en série, qui effectuent les travaux suivants :
        \begin{itemize}
            \item construction du chemin pointant vers les fiches techniques (sur la base des uid)
            \item construction d'une feature contenant les données des fichiers, en binaire
            \item construction du texte complet de la fiche technique (en se basant sur la library pdfminer.six)
        \end{itemize}
        Le résultat du lancement de ce pipeline est présenté à la \reftable{tbl:mod_GT_fulltexts}.

        {\renewcommand{\arraystretch}{1.5}%
        \begin{table}[htbp]
            \begin{center}
            {\scriptsize
            \input{tbls/processed_FT.tex}
            }
            \caption{Exemples du contenu de fiches techniques au format texte (tronqués)}
            \label{tbl:mod_GT_fulltexts}
            \end{center}
        \end{table}
        }        

        \section{Découpage des textes en blocs}

        Le second travail est le découpage des textes en blocs. 
        Dans un premier temps, on va simplement effectuer ce découpage en splittant le texte lorsque deux retours à la ligne successifs sont détectés.
        Un exemple de découpage est présenté ci-dessous.

        \begin{multicols}{3}
        \begin{spacing}{1.0}
        \label{blocks_examples}
        {\tiny
        \input{tbls/block_example.tex}
        }
        \end{spacing}
        \end{multicols}

        On constate que le découpage n'est pas idéal, cf. la fiche technique de ce produit, présentée en annexe \mref{ex:FT_meltrappeur}.
        Les séparations des cellules des tableaux de cette fiche ne sont pas prises en compte, et on a des blocs trop étendus.

        \section{Train/Test split}

        Dans la mesure où l'on possède assez peu de données, on va conserver un échantillon assez important dans le jeu d'entraînement : 400 produits (soit 80\% des données disponibles).

        \section{Entraînement du modèle}

        On fait tourner de la même manière que sur le modèle dit \og ouvert \fg, à savoir qu'on ne préprocesse pas les données avant d'appliquer le CountVectorizer.

        \section{Illustration des prédictions obtenues}

        Un échantillon des prédictions obtenues est présenté dans la \reftable{tbl:GT_prediction_sample}.
        Pour éviter d'avoir des listes d'ingrédients prédites prenant trop de place dans cette table, celles dont la longueur dépasse 500 caractères ont été filtrées avant génération de cet échantillon.
        Les résultats présentés à cette table sont donc vraisemblablement biaisés, dans la mesure où les très longues listes prédites doivent avoir plus de chance d'être erronées.

        Les grandes tendances qui se dégagent à l'analyse de cette liste sont les suivantes : 
        \begin{itemize}
            \item globalement, les résultats sont bons. On retrouve régulièrement des morceaux de texte qui sont similaires à la liste cible
            \item une erreur qui revient régulièrement est le fait que le découpage en blocs est parfois imparfait, on sélectionne \og trop large \fg
            \item à l'inverse, le modèle n'a pas retiré des listes d'ingrédients prédites des mentions qui ont été rétirées lors de l'étiquetage manuel (cf. les règles d'annotation présentées en annexe \mref{annotation_rules}) : les préfixes de type \og Liste d'ingrédients : \fg, les allégations telles que \og Teneur totale en sucres : 60g pour 100g \fg \dots
            \item le modèle semble plus perfomant lorsque la liste d'ingrédients réelle est longue. On le vérifiera dans le chapitre relatif à la mesure de la performance du modèle
        \end{itemize}

        Un mot sur les cas où la liste d'ingrédients cible ou prédite sont vides :
        \begin{itemize}
            \item Les listes d'ingrédients cible sont vides lorsque la pièce jointe ne mentionnait pas de liste d'ingrédients. Cela peut arriver, et les produits concernés n'ont pas été sortis de l'échantillon. Il est 
            important de pouvoir aussi mesurer les faux positifs, qui sont nombreux avec cette techniques de choix systématique du meilleur candidat
            \item Les liste d'ingrédient prédites sont vides lorsque l'outil de parsing des pdf (pdfminer.six) n'a extrait aucun texte. C'est le cas quand la pièce jointe était un document imprimé qui a été scanné. Le texte n'est présent que sous forme d'image (cf. la fiche technique du sel en annexe \mref{ex:FT_sel})
        \end{itemize}

        {\renewcommand{\arraystretch}{1.5}%
        \begin{spacing}{1.0}
        \begin{center}
            {\scriptsize
            \input{tbls/GT_prediction_sample.tex}
            }
        \end{center}
        \end{spacing}
        }

    \chapter{Mesure de la performance}
    
    Comme vu aux chapitre précédents, il est indispensable de mesurer la performance de nos modèles.
    On le fera sur le modèle se basant sur les données manuellement étiquetées.
    Le principe est présenté à la \reffig{fig:measured_model}.

    \begin{figure}[htbp]
        \begin{center}
        \includegraphics[width=0.9\linewidth]{img/measured_model.png}
        \end{center}
        \caption{Illustration de la méthodologie de mesure de la performance}
        \label{fig:measured_model}
    \end{figure}     

        \section{Accuracy}
        
        La métrique qui tombe le plus sous le sens est l'accuracy (on utilisera le terme anglais pour éviter les confusions avec la notion de \og precision \fg telle qu'elle est utilisée par exemple dans le f1-score).
        On mesure simplement la proportion de prédictions qui sont égales à la ground truth.
        La méthodologie utilisée est détaillée dans le notebook \og Mesure de la performance \fg présenté en annexe \mref{code:performance_measurement}.

            \subsection{Approche naïve}

                \subsubsection{Description de cette approche}

                L'approche \og naïve \fg consiste simplement à mesurer la proportion de textes prédits strictement égaux à la ground truth.
                Or, comme cela a été vu précédemment : 
                \begin{itemize}
                    \item les règles d'étiquetage manuel, détaillées à l'annexe \mref{annotation_rules}, montrent que des transformations sont parfois appliquées au contenu des listes d'ingrédients des pièces jointes avant d'établir la ground truth
                    \item les textes à comparer sont longs (jusqu'à quelques centaines de caractères), cf. \reftable{tbl:GT_prediction_sample}
                    \item la mise en forme, en particulier les retours à la ligne, ne sont pas positionnés aux mêmes endroits. Dans le parsing des documents pdf, lorsque le texte revient à la ligne après avoir atteint le bord de la page, on a un retour à la ligne. Ce comportement n'a pas été reproduit dans l'établissement de la ground truth
                    \item le découpage en blocs de texte, de manière simple, produit des textes qui ne sont pas toujours le reflet du contenu spatialisé de ce document (cf. l'exemple donné à la section \mref{blocks_examples}, et la fiche technique associée en annexe \mref{ex:FT_meltrappeur})
                \end{itemize}
                On s'attend donc à avoir une \og accuracy naïve \fg faible.

                \subsubsection{Les résultats obtenus}

                Comme présenté dans notebook \og Analyse de la performance \fg (cf. annexe \mref{code:performance_measurement}), les résultats obtenus sont conformes à l'attendu : l'accuracy est très faible.

                Elle vaut 1\% (1 échantillon sur 100 produits) lorsqu'on la mesure après 

                \subsubsection{Nécessité d'améliorer cette métrique}

            \subsection{Avec du \og text-postprocessing \fg}
        
            On assouplit un peu les contraintes en effectuant du text processing, à la fois sur la ground truth et les résultats du modèle.
            On tokenize (pour retirer ponctuation, \og blancs \fg), et on retire la casse et les accents pour constituer des textes qui ont plus de chance d'être égaux.
            Ce n'est pas une astuce pour améliorer les résultats, c'est juste que cette manière de mesurer est plus pertinente.
            Illustration des résultats en applicant diverses manières de \og post-processer \fg.

        \section{Fonctions de \og loss \fg spécifiques}

        On peut aussi définir des fonctions de loss, qui permettent d'être plus fin qu'une simple évaluation OK / KO du résultat du modèle.
        On calculera une distance entre le résultat du modèle, et de la ground truth.

            \subsection{Distance de Levenshtein}

            Brève description de chacune de ces distances.

            \subsection{Distance de Dameray-Levenshtein}

            Brève description de chacune de ces distances.
            
            \subsection{Distance de Jaro}
            Brève description de chacune de ces distances.

            \subsection{Distance de Jaro-Wrinkler}
            Brève description de chacune de ces distances.

            \subsection{Métriques non retenues}
            Distance de Hamming

            \subsection{Conclusion sur la métrique à utiliser}
            Une fois que cela aura été fait.

    \chapter{Transfer learning}
        
        \section{Principe du pré-entraînement}
        
        Expliquer qu'il s'agit d'une approche hybride des 2 modèles précédents
        On effectue une entraînement à la fois sur une partie des listes d'ingrédients du PIM, et sur une partie des données étiquetées.
        On vérifie ensuite, uniquement sur 

        \section{Illustration de l'impact sur la performance}

        Ici, on montre l'impact sur la performance, du fait d'intégrer des listes d'ingrédients.
        On met en abscisse le nombre de listes d'ingrédients qu'on ajoute, et en ordonnée la performance du modèle (avec barre d'erreurs, via cross validation).
        On regarde si l'effet est positif : cela montrera s'il est intéressant d'avoir plus de données.
        On regarde si on observe une saturation : cela montrera si on a déjà suffisamment de données sous formes de listes d'ingrédients dans le PIM, ou bien si ce serait intéressant d'en acquérir plus.

    \chapter{Hyperparameter tuning}
            
    On peut, dans l'optique d'améliorer la performance du modèle, ajuster certains paramètres et d'évaluer l'impact via une grid search.
    On fera tourner sur le modèle avec transfer learning.

        \section{Les paramètres ajustables}

            \subsection{La prise en compte des \og n-grams \fg dans la tokeinzation}

            On peut utiliser les n-grams lors de la tokenisation.

            \subsection{L'application de \og n-grams \fg de blocs}

            Voir si dans la recherche du meilleur candidat, on s'autorise la constitution de \og n-grams \fg de blocs.

            \subsection{L'utilisation d'expressions régulières dans le split des blocs}

            Voir si certaines expressions régulières pour splitter les blocs procurent de meilleurs résultats.

            \subsection{Applications d'autres fonctions de similarité}

            Voir l'impact d'utiliser d'autres manières de calculer la similarité.

            1 - autre chose que la similarité cosinus (fonction du nombre de mots du bloc et de la proportion de mots issus du vocabulaire des ingrédients)

            2 - en appliquant du TF et du TF-IDF

        \section{Application d'une grid search}

        Illustrer ici les résultats d'une grid search ou d'une random search si trop gourmand.