\documentclass{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

\usepackage{lmodern}

\usepackage{setspace}
\spacing{1.5}

% \makeatletter
% \@addtoreset{chapter}{part}
% \makeatother  

\usepackage{url}

\usepackage{graphicx}

\usepackage{array,multirow,makecell,tabularx}
\renewcommand\tabularxcolumn[1]{>{\centering\arraybackslash}m{#1}}
% \newcolumntype{Y}{>{\centering\arraybackslash}X}
% \setcellgapes{1pt}
% \makegapedcells
% \newcolumntype{R}[1]{>{\raggedleft\arraybackslash }b{#1}}
% \newcolumntype{L}[1]{>{\raggedright\arraybackslash }b{#1}}
% \newcolumntype{C}[1]{>{\centering\arraybackslash }b{#1}}

\newcommand{\emphbox}[1]{
    \vspace{5mm}
    \noindent\fbox{
        \begin{minipage}[c]{\linewidth}
            {\em #1}
        \end{minipage}
    }
}

\title{Extraction de données relatives aux produits alimentaires à partir de documents non structurés}
\author{Pierre \textsc{massé}}
\date{Juin 2020}

\begin{document}

\maketitle

\large
\begin{abstract}
    {\em
    
    La gestion de l'information produit est devenu un enjeu de société majeur ces dernières années.
    Les scandales sanitaires récents ont déclenché une prise de conscience collective des consommateurs, en parallèle de la mise en place de réglementations de plus en plus contraignantes pour l'ensemble des acteurs de la filière\cite{incotext}\cite{incoexpl}.
    \`{A} ce titre, le Groupe Pomona a lancé ces dernières années un projet majeur de refonte des processus et des outils de gestion de l'information produit.

    La première filiale du Groupe a fait l'objet d'un déploiement réussi, mais cela a toutefois mis en évidence le fait que des gains à la fois en qualité et en productivité restent accessibles.

    La mise en place d'outils mettant en oeuvre les principes du Machine Learning appliqués au traitement du langage permettrait d'aider les opérationnels de la gestion de l'information à interpréter plus vite et mieux les documents mis à disposition par les fournisseurs du Groupe.

    Le présent rapport détaille la mise en place d'un outil permettant d'extraire les listes d'ingrédients des fiches techniques transmises par les fabricants des produits.
    }
\end{abstract}
\normalsize

\tableofcontents

\input{business}

\part{Les données}
    \chapter{Le périmètre produit}
        \section{Accessibilité de la donnée en fonction des branches}
        \section{Les branches déployées}
        \section{Les types de produit}
    \chapter{Les données utilisables}
        \section{Données structurées}
        
        \section{Données non structurées}
        
        Les listes d'ingrédients juste une liste ordonnées d'ingrédients triés par ordre décroissant de quantité mise en oeuvre.

        Parfois détaillé par phase, mais en général déconseillé.
        \section{Pièces jointes}
            \label{pieces_jointes}

            Dans chacune des sections, mentionner la volumétrie de données accessibles (avec les facettes migration, statuts, \& compagnie) et tout

            \subsection{Fiches techniques fournisseur}
            \subsection{\'{E}tiquettes produit}
            \subsection{Fiches logistiques fournisseur}
            \subsection{Fiches techniques et argumentaires Pomona}
        \section{Récapitulatif de la complétude des données}

        Mettre ici un ou plusieurs tableaux récapitulatifs illustrant les données possédées quantitativement.

        \section{Analyse qualitative des données}
        
        Montrer qu'un sondage basique fait que la qualité actuelle est perfectible

        Mettre également la distribution numérique des produits par fournisseur et insister sur la difficulté posée par de multiples formats

        Dire ici qu'il y a finalement beaucoup de pdf qui possèdent des textes extractibles vs. uniquement des images.

        \section{Les données \og manuellement étiquetées \fg}

        Montrer comment elles ont été produites

        Expliciter les règles de gestion qui ont été listées pendant l'étiquetage manuel

        Evaluer la cohérence entre étiquettes manuelles et contenu du PIM

\part{Les objectifs de ce projet}
    \chapter{Les cas d'usage}
        \section{Objectifs : Qualité et productivité}
        \section{La préalimentation d'information}
        \section{Le contrôle à la saisie fournisseur}
        \section{L'aide aux vérifications Pomona}
        \section{Les contrôles en masse asynchrones}
    \chapter{Les types de données à récupérer}
        \section{La composition produit}
        \section{Les données nutritionnelles}
        \section{Les données logistiques}
    \chapter{Le choix du cas d'usage}
        \section{Les multiples formats}
        \section{Les informations \og spatialisées \fg}
        \section{La complexité dans la réprésentation des données logistiques}
        \section{La moindre représentation des étiquettes}
        blablabla
        \newline
        \newline
        \emphbox{Au vu des différentes contraintes listées plus haut, on s'attachera à extraire \emph{les listes d'ingrédients} des produits \emph{alimentaires} de la branche \emph{EpiSaveurs} depuis \emph{les fiches techniques fournisseur}, en se basant sur \emph{le contenu textuel} de ces documents.}

\part{Construction du modèle}
    \chapter{Les principes généraux}
        \section{Contenu du texte d'une liste d'ingrédients}

        En général, chaque ingrédient sera présent une seule fois dans la liste.

        Le calcul d'embeddings via des modèles tels que SVD ou Word2Vec fait peu de sens.
        \newline
        \newline
        \emphbox{l'extraction des textes se fait au format \emph{Bag Of Words}, sans utiliser de notion d'IDF. L'utilsation de TF semble églament sujette à caution.}

        \section{Limitation à l'identification des listes d'ingrédients}

        On est sur une taxonomie d'informations limitée dans les fiches techniques.

        On pourrait envisager de classifier l'ensemble des textes présents dans les fiches techniques.

        Mais l'absence de données étiquetées rend cette tâche impossible. La charge d'étiquetage d'un nombre représentatif de blocs de texte de fiches techniques est trop importante pour être mise en oeuvre dans le cadre de ce projet.

        \section{Conversion de documents en texte}
        
        dire ici qu'on utilise principalement pdfminer vs. d'autres outils d'OCR.

        De plus, on partira dans un premier temps sur une transformation basique d'un document en texte, sans passer par une analyse de la localisation des textes sur le document.
            
    \chapter{Construction d'un modèle simple \og ouvert \fg}
        
    Expliciter le principe de ce modèle avec un schéma simple.

    Pas de mesure possible de la performance

        \section{Extraction des données}

        Ne garder que produits d'épicerie et boissons non alcoolisées

        \section{Conversion en blocs de texte}
        \section{Train/Test split}
        \section{Entrainement du modèle}
        \section{Calcul de la similarité}
        \section{Illustration des résultats obtenus}
    
    \chapter{Utilisation des données manuellement étiquetées}

    Expliciter pourquoi on ne peut pas faire tourner (référence parties précédentes) sur l'ensemble des données
        
        \section{Chargement des données manuellement étiquetées}

        \section{Train/Test split}

        \section{Entraînement du modèle}

        \section{Illustration des prédictions obtenues}


    \chapter{Mesure de la performance}
    
        \section{Précision}
            \subsection{Approche naïve}

            \subsection{Avec du \og text-postprocessing \fg}
        
        \section{Similarité cosinus}

        \section{Fonction de \emph{loss} spécifique}

        Expliciter les diverses distances, et pourquoi certaines sont plus pertinentes que d'autres.

        Ex : on ne garde pas la distance de Hamming
            \subsection{Distance de Levenshtein}

            \subsection{Distance de Dameray-Levenshtein}

            \subsection{Distance de Jaro}

            \subsection{Distance de Jaro-Wrinkler}
    
        \section{Cross-validation des modèles précédents}
            
            \subsection{Modèle \og ouvert \fg}

            \subsection{Modèle entraîné sur les données étiquetées manuellement}

    \chapter{Transfer learning}
        
        \section{Principe du pré-entraînement}
        
        Expliquer qu'il s'agit d'une approche hybride des 2 modèles précédents

        \section{Illustration de l'impact sur la performance}


    \chapter{Hyperparameter tuning}
            
        \section{Les paramètres ajustables}

        \section{Application d'une grid search}

\part{Travaux subséquents}
    \chapter{Opérationnalisation de cette maquette}    
        \section{Client et sponsor métier}
        \section{Définition des règles de gestion}
        \section{Mise en place d'une organisation projet}
        \section{Industrialisation du code}
        Prochaines étapes : opérationnalisation via API \\
        Documentation
        \section{Monitoring de la performance du modèle}
    \chapter{Extension des fonctionnalités offertes}
        \section{Prise en compte de nouveaux types de pièces jointes}
        \section{Utilisation d'outil d'OCR pour les pdf non structurés}
        \section{Mise en place d'outil de spatialisation des textes}
        \section{Construction d'outils d'extraction de données connexes à la composition}
        \section{\'{E}largissement aux données nutritionnelles}
        \section{Extraction \og opportuniste \fg d'informations \\ complémentaires}
        \section{\'{E}valuation de la performances sur d'autres familles de produits}


\appendix
\part{Figures, tableaux et bibliographie}
    \listoftables
    \listoffigures
    \bibliographystyle{plain}
    \bibliography{./biblio}
\part{Exemple de documents fournisseur}
    \chapter{Fiches techniques}
    \chapter{\'{E}tiquettes produit}
\part{Le code utilisé}
    \chapter{Extraction de données du PIM}
    \chapter{Conversion des pièces jointes en textes}
    \chapter{Identification des listes d'ingrédients}

\end{document}