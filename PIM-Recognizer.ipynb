{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "## Goal\n",
    "\n",
    "This notebook aims to present how data stored in non formatted documents could be leveraged to improve data quality inside the PIM.\n",
    "\n",
    "This notebook uses a handful of modules developped inside this project.\n",
    "\n",
    "## What pipeline?\n",
    "\n",
    "The different steps for this project are as follows:\n",
    "\n",
    "1. fetch all product IDs from PIM with the associated ingredient lists\n",
    "- split the products between a train set and a test set\n",
    "- train the algorithm on the train set: i.e. construct the vocabulary\n",
    "- make it make prediction on the test set\n",
    "- compare it with the ingredient list on this product\n",
    "\n",
    "# 1. Fetching the data\n",
    "\n",
    "We will use production data for training and testing of this model. The ID of the products are the PIM uid, and therefore are listed in the directory of the PIM-API module.\n",
    "\n",
    "First, let's get those uids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import pimapi\n",
    "requester = pimapi.Requester('prd')\n",
    "requester.refresh_directory()\n",
    "requester._directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see the modification status of the product via the `modification_report` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requester.modification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outdated products can be refreshed via the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requester.fetch_list_from_PIM(requester.modified_items(), batch_size=20)\n",
    "requester.dump_data_from_result()\n",
    "requester.dump_files_from_result()\n",
    "requester.modification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PIM uids of the products are the keys of the `directory` of our requester. We extract the ingredients associated with these uids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requester.fetch_all_from_PIM(page_size=1000, max_page=-1, nx_properties='*')\n",
    "requester.result[0].json()['entries'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'uid': 'uid', 'Libellé': 'title', 'Ingrédients': 'properties.pprodc:ingredientsList'}\n",
    "df = requester.result_to_dataframe(record_path='entries', mapping=mapping, index='uid')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train / Test split\n",
    "\n",
    "We will separate our data into a train test and a test set of equal sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "X_train.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "X_test.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Constructing the vocabulary\n",
    "\n",
    "We will now use bag-of-words related functionalities of scikit-learn to construct our vocabulary.\n",
    "\n",
    "## 3.1 Removing `None` values\n",
    "\n",
    "First step is to remove `None` values from ingredient lists to make our count of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f'None values before replacement in X_train: {sum(pd.isna(X_train[\"Ingrédients\"]))}')\n",
    "X_train.loc[:, 'Ingrédients'].fillna('', inplace=True)\n",
    "print(f'None values after replacement in X_train: {sum(pd.isna(X_train[\"Ingrédients\"]))}')\n",
    "print(f'None values before replacement in X_test: {sum(pd.isna(X_test[\"Ingrédients\"]))}')\n",
    "X_test.loc[:, 'Ingrédients'].fillna('', inplace=True)\n",
    "print(f'None values after replacement in X_train: {sum(pd.isna(X_test[\"Ingrédients\"]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Parsing the corpus\n",
    "\n",
    "We now parse our ingredient lists, with a naive approach (no stop words, no preprocessing, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train.loc[:, 'Ingrédients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we know have a matrix with as much rows as the number of products in our train corpus, and as much columns as the number of different words in their ingredient lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary has been computed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The vocabulary length is {len(count_vect.vocabulary_)}')\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the top 10 most frequent words in our ingredient lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = X_train_counts.sum(axis=0)\n",
    "word_counts2 = [(word, word_counts[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "word_counts2.sort(key=lambda x: x[1], reverse=True)\n",
    "word_counts = word_counts2\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. First analysis of a single document\n",
    "\n",
    "## 4.1 Parsing a doc from the test set\n",
    "\n",
    "First, we use a function that parses a document from the disk from its product uid and returns a list of strings. For illustration, we choose one of the products in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pimpdf import PDFDecoder\n",
    "import os\n",
    "\n",
    "# This uid has been gotten from the previous cell, maybe from a previous run!\n",
    "uid = '776613db-a461-44e1-ab6a-1344ac6ae99c'\n",
    "test_doc_blocks = PDFDecoder.path_to_blocks(os.path.join('.', 'dumps', 'prd', uid, 'FTF.pdf'))\n",
    "print(f'Number of blocks in this document: {len(test_doc_blocks)}')\n",
    "test_doc_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific document (*776613db-a461-44e1-ab6a-1344ac6ae99c*), the correct block of text is:\n",
    "\n",
    "    Ingrédients:  sirop de glucose; sucre; gélatine; dextrose; acidifiants: acide citrique, acide malique; agent d'enrobage: cire de carnauba; correcteurs d'acidité: citrate tricalcique, malate acide de sodium; arôme; concentrés de fruits et de plantes: citron, carthame, spiruline, patate douce, radis; sirop de sucre inverti; colorants: carmins, bleu patenté V, carotènes végétaux, lutéine, anthocyanes.\n",
    "\n",
    "The index of this correct block is *9* in our block list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parse these blocks with the vocabulary computed from our train set. We reuse the `CountVectorizer` we trained before, but take care just to use the `transform` method.\n",
    "\n",
    "Using the `fit_transform` method would retrain the model with the current blocks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_counts = count_vect.transform(test_doc_blocks)\n",
    "test_doc_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Getting some insights from this first analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute and draw the terms counts for our blocks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "term_counts = np.ravel(test_doc_counts.sum(axis=1))\n",
    "term_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the *true* ingredient list has the higher term_count. However, term counts alone are likely to have a biais toward long blocks, so we can also compute a term frequency.\n",
    "\n",
    "We will instantiate a new count_vectorizer, for the sole purpose of counting tokens in the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_word_counts = np.ravel(CountVectorizer().fit_transform(test_doc_blocks).todense().sum(axis=1))\n",
    "blocks_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the frequencies of \"ingredient words\" in the blocks of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs = np.divide(term_counts, blocks_word_counts, out=np.zeros(term_counts.shape), where=blocks_word_counts!=0)\n",
    "term_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "colors = ['other'] * len(test_doc_blocks)\n",
    "colors[true_idx] = 'ingredients'\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n",
    "sns.barplot(ax=ax[0], x=term_counts, y=[block[:10] for block in test_doc_blocks], hue=colors, dodge=False)\n",
    "sns.barplot(ax=ax[1], x=term_freqs, y=[block[:10] for block in test_doc_blocks], hue=colors, dodge=False)\n",
    "ax[0].set_title('Terms counts', fontsize=20)\n",
    "ax[0].set_ylabel('Blocks', fontsize=18)\n",
    "ax[1].set_title('Terms frequencies', fontsize=20)\n",
    "ax[1].xaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some very short texts also have an \"ingredient word frequency\" equal to 100%.\n",
    "\n",
    "We can draw a scatter plot of these indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n",
    "sns.scatterplot(ax=ax, x=term_counts, y=term_freqs, hue=colors)\n",
    "ax.set_title('Ingredient word frequency vs. count', fontsize=20)\n",
    "ax.set_xlabel('Ingredient word count in block', fontsize=16)\n",
    "ax.set_ylabel('Ingtredient word frequency in block', fontsize=16)\n",
    "ax.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the correct ingredient list is in the top right quadrant of this representation. Should all cases yield results as sharply contrasted, we will have no difficulty in constructing an accurate functionality!\n",
    "\n",
    "## 4.3 A difficulty arises in assessing ground truth for model validation\n",
    "\n",
    "### Long strings are seldom strictly equal\n",
    "\n",
    "We can compare the document ingredient list, with the one stored in the PIM system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_blocks[true_idx].replace('\\n', '') == df.loc[uid, \"Ingrédients\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a great deal of the current data in the PIM system has (at least once!) been manually keyed in, it is very likely that there will be an arguably high ratio of mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('From pdf file: ')\n",
    "print(test_doc_blocks[true_idx].replace('\\n', ''))\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "print('From PIM system:')\n",
    "print(df.loc[uid, \"Ingrédients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that although very close, those 2 texts are somewhat different, from the punctuation marks.\n",
    "\n",
    "### Different strategies can be undertaken to get around this difficulty\n",
    "\n",
    "It is mandatory to compare the results of the model with the ground truth to assess the performance of the model. Some workarounds can be set up:\n",
    "\n",
    "- Ignoring all products that do not have a strict matching block in their pdf file:\n",
    "    - This will enable for a simple validation process\n",
    "    - But it might dramatically decreasing the size of our dataset\n",
    "    - as well as making 'short ingredient list' product overrepresented\n",
    "    \n",
    "    \n",
    "- Defining a softer way to match texts between pdf files and PIM system data **with some text preprocessing** and filtering products that do not have a matching block\n",
    "    - This will mitigate the previous drawbacks\n",
    "    - But will increase complexity\n",
    "    \n",
    "    \n",
    "- Defining a softer way to match texts between pdf files and PIM system data **by computing an edition distance** and filtering products that do not have a matching block\n",
    "    - This will mitigate the previous drawbacks\n",
    "    - But will increase complexity, as well as requiring to manually set up a distance threshold.\n",
    "    - This could also lead to have separate pdf file blocks considered ground truth should the threshold distance be too high\n",
    "    \n",
    "- Considering blocks n-grams \n",
    "    - This might increase the number of \n",
    "\n",
    "- Manually labeling some pdf files\n",
    "    - The most efficient\n",
    "    - But the most time-consuming too!\n",
    "\n",
    "\n",
    "# 5. Comparison between PIM system *ground truth* with documents content\n",
    "\n",
    "We can try to find the products for which the PIM system ingredient list is strictly equal to one of the pdf file blocks.\n",
    "\n",
    "## 5.1 Retrieving all the blocks from our corpus\n",
    "\n",
    "The function below enables to retrieve all the blocks as a pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uid_list = list(requester._directory.index)\n",
    "path_list = [os.path.join('.', 'dumps', 'prd', uid, 'FTF.pdf') for uid in uid_list]\n",
    "path_series = pd.Series(path_list, index=uid_list)\n",
    "blocks_series = PDFDecoder.threaded_paths_to_blocks(path_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_series.rename('pdf_blocks', inplace=True)\n",
    "blocks_series.index.rename('uid', inplace=True)\n",
    "blocks_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it takes some time to run the pdf parsing on all the corpus, we save it in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "blocks_series.to_csv(os.path.join('.', 'dumps', 'prd', 'blocks_'+ timestamp + '.csv'), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Comparing blocks with PIM ingredient lists\n",
    "\n",
    "We only keep products with ingredient list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingred = df.loc[pd.notna(df['Ingrédients'])]\n",
    "df_ingred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among these products with an ingredient list, we only keep the ones which have (at least) a block from the pdf that is strictly matching its PIM ingredient list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joined_df = df_ingred.join(blocks_series)\n",
    "matching = joined_df.loc[joined_df['Ingrédients'] == joined_df['pdf_blocks']].drop_duplicates()\n",
    "matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only keeping products with strict equality between PIM ingredient list and any of the pdf blocks has dramatically reduced their number: from 9549 to 299. New sample size only represents less than 3.5% of the initial set.\n",
    "\n",
    "## 5.3 Making the same comparison but softening the criterion\n",
    "\n",
    "TODO !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Manually labeling data\n",
    "\n",
    "The most straightforward solution to have labeled data (but also the most time consuming...) is to label them manually!\n",
    "\n",
    "## 6.1 Randomly selecting products to label\n",
    "\n",
    "First step is to **randomly** select products from PIM, extract their attached documents and store them away safely. The criterion for these 500 products will be:\n",
    "\n",
    "- they are food products: beverages or grocery\n",
    "- they have a supplier technical datasheet attached\n",
    "\n",
    "They will be stratified by product type (beverage or grocery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requester.result[0].json()['entries'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'uid': 'uid',\n",
    "           'designation': 'title',\n",
    "           'state': 'state',\n",
    "           'ingredients': 'properties.pprodc:ingredientsList',\n",
    "           'type': 'properties.pprodtop:typeOfProduct'}\n",
    "df = requester.file_report_from_result(mapping=mapping, index='uid') # , record_path='entries') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.loc[(df.type.isin(['grocery', 'nonAlcoholicDrink']))\n",
    "                     & (df.has_supplierdatasheet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, ground_truth_df = train_test_split(filtered_df, test_size=500, random_state=42, stratify=filtered_df.type)\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our products, we save their attached documents on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requester.fetch_list_from_PIM(ground_truth_df.index, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requester.dump_data_from_result(update_directory=False, root_path=os.path.join('.', 'ground_truth'))\n",
    "requester.dump_files_from_result(update_directory=False, root_path=os.path.join('.', 'ground_truth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df['designation'].to_csv(os.path.join('.', 'ground_truth', 'uid.csv'), header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then try to reimport the ground truth after having processed some files, just to check that everything went ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join('.', 'ground_truth', 'manually_labelled_ground_truth.csv'),\n",
    "            sep=';',\n",
    "            encoding='latin-1',\n",
    "            index_col='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = '70500268-802d-4211-93ba-9edbf6e0e7a3'\n",
    "print(pd.read_csv(os.path.join('.', 'ground_truth', 'manually_labelled_ground_truth.csv'), sep=';', encoding='latin-1').set_index('uid', drop=True).loc[uid, 'ingredients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Reconstructing Train/Test split after labelling\n",
    "\n",
    "We will now reimport the ground truth csv file and recreate the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set =pd.read_csv(os.path.join('.', 'ground_truth', 'manually_labelled_ground_truth.csv'),\n",
    "                      sep=';',\n",
    "                      encoding='latin-1',\n",
    "                      index_col='uid')\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'uid': 'uid',\n",
    "           'Libellé': 'title',\n",
    "           'Ingrédients': 'properties.pprodc:ingredientsList'}\n",
    "df = requester.result_to_dataframe(record_path='entries', mapping=mapping, index='uid')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[(~df.index.isin(test_set.index)) & (pd.notna(df['Ingrédients']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train.loc[:, 'Ingrédients'])\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different strategies\n",
    "\n",
    "TODO !!!\n",
    "\n",
    "But The similarity between the ground truth (the pdf file) and the content of the PIM system can be measured via the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). This distance is the number of character insertions, deletions or substitutions to get from one text to the other.\n",
    "\n",
    "If we compute this distance between the pdf file block and the PIM system content we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "dist = jellyfish.levenshtein_distance(test_doc_blocks[true_idx].replace('\\n', ''),\n",
    "                                      df.loc[uid, \"Ingrédients\"])\n",
    "print(f'Levenshtein distance between pdf file and PIM system content is: {dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute this distance for each block in our pdf file, and plot it in a bar graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = list(map(lambda x:jellyfish.levenshtein_distance(x.replace('\\n', ''), df.loc[uid, \"Ingrédients\"]),\n",
    "                 test_doc_blocks))\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.barplot(ax=ax, x=distances, y=[block[:10] for block in test_doc_blocks], hue=colors, dodge=False)\n",
    "ax.set_title('Levenshtein distance by block', fontsize=20)\n",
    "ax.set_ylabel('Blocks', fontsize=18)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Making prediction on a document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
